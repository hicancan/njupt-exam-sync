import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import os
import re
import urllib3
from typing import Optional

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- é…ç½®åŒºåŸŸ ---
LIST_URL = "https://jwc.njupt.edu.cn/1594/list.htm"
SAVE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "public", "data")

# 1. å¿…é¡»åŒ…å«çš„å…³é”®è¯ (ä¸”å…³ç³»)
REQUIRED_KEYWORDS = ["å­¦å¹´", "å­¦æœŸ"]
# 2. å¿…é¡»åŒ…å«å…¶ä¸­ä¹‹ä¸€çš„å…³é”®è¯ (æˆ–å…³ç³»)
TARGET_KEYWORDS = ["è€ƒè¯•å®‰æ’è¡¨", "æœŸæœ«è€ƒè¯•", "è¯¾ç¨‹ç»“æŸè€ƒè¯•"]
# 3. ç»å¯¹ä¸èƒ½åŒ…å«çš„å…³é”®è¯ (æ’é™¤å™ªéŸ³)
EXCLUDE_KEYWORDS = [
    "é˜¶æ®µæ€§", "è¡¥è€ƒ", "æ¸…æ¬ ", "åˆ†çº§", "è¡¥å­¦", "æ¢è¯", 
    "é‡ä¿®", "é€‰æ‹”", "ç«èµ›", "å‘è½¦", "ç›‘è€ƒ"
]

# ä¼ªè£…è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://jwc.njupt.edu.cn/"
}

def is_valid_title(title: str) -> bool:
    """
    åˆ¤æ–­æ ‡é¢˜æ˜¯å¦ä¸ºæ­£è§„æœŸæœ«è€ƒè¯•å®‰æ’
    """
    # 1. æ£€æŸ¥æ’é™¤è¯
    for kw in EXCLUDE_KEYWORDS:
        if kw in title:
            return False
    
    # 2. æ£€æŸ¥å¿…é¡»åŒ…å«çš„è¯
    for kw in REQUIRED_KEYWORDS:
        if kw not in title:
            return False
            
    # 3. æ£€æŸ¥ç›®æ ‡è¯ (è‡³å°‘å‘½ä¸­ä¸€ä¸ª)
    if not any(kw in title for kw in TARGET_KEYWORDS):
        return False
        
    return True

def is_student_file(filename: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºå­¦ç”Ÿç”¨è¡¨"""
    return "å­¦ç”Ÿ" in filename

def is_teacher_file(filename: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ•™å¸ˆ/ç›‘è€ƒè¡¨"""
    keywords = ["ç›‘è€ƒ", "æ•™å¸ˆ", "å·¡è€ƒ", "æ•™åŠ¡å‘˜"]
    return any(kw in filename for kw in keywords)

def download_file(url: str, save_path: str) -> bool:
    try:
        print(f"  â¬‡ï¸  ä¸‹è½½ä¸­: {os.path.basename(save_path)} ...", end="", flush=True)
        # verify=False is intentional: JWC often has self-signed/incomplete cert chains
        response = requests.get(url, headers=HEADERS, verify=False, timeout=30)
        response.raise_for_status()
        with open(save_path, 'wb') as f:
            f.write(response.content)
        print(" [å®Œæˆ]")
        return True
    except Exception as e:
        print(f" [å¤±è´¥] {e}")
        return False

def find_latest_schedule_notification() -> Optional[tuple[str, str]]:
    """éå†åˆ—è¡¨é¡µï¼Œå¯»æ‰¾æœ€æ–°çš„ã€ç¬¦åˆé€»è¾‘çš„é€šçŸ¥"""
    print(f"ğŸ” è®¿é—®é€šçŸ¥åˆ—è¡¨: {LIST_URL}")
    try:
        resp = requests.get(LIST_URL, headers=HEADERS, verify=False, timeout=10)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        container = soup.select_one('div.col_news_con')
        if not container:
            print("âŒ æœªæ‰¾åˆ°åˆ—è¡¨å®¹å™¨ï¼Œè¯·æ£€æŸ¥é€‰æ‹©å™¨ã€‚")
            return None

        news_items = container.select('li.news')
        
        for item in news_items:
            # æå–æ ‡é¢˜
            title_span = item.select_one('span.news_title')
            a_tag = title_span.find('a') if title_span else item.find('a')
            
            if not a_tag: continue
            
            title = a_tag.get('title') or a_tag.get_text(strip=True)
            link = urljoin(LIST_URL, a_tag.get('href'))
            
            # ä½¿ç”¨å¢å¼ºçš„é€»è¾‘åˆ¤æ–­æ ‡é¢˜
            if is_valid_title(title):
                print(f"âœ… å‘½ä¸­ç›®æ ‡: [{title}]")
                print(f"ğŸ”— é“¾æ¥åœ°å€: {link}")
                return link, title
            else:
                # å¼€å¯æ­¤è¡Œå¯æŸ¥çœ‹è¢«è¿‡æ»¤æ‰çš„æ ‡é¢˜ï¼ˆè°ƒè¯•ç”¨ï¼‰
                # print(f"   è·³è¿‡: {title}")
                pass
        
        print("âš ï¸ æœªåœ¨é¦–é¡µæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æœŸæœ«è€ƒè¯•é€šçŸ¥ã€‚")
        return None
        
    except Exception as e:
        print(f"âŒ åˆ—è¡¨è·å–å¤±è´¥: {e}")
        return None

def process_detail_page(url: str, title: str):
    """è§£æè¯¦æƒ…é¡µå¹¶æ™ºèƒ½ä¸‹è½½é™„ä»¶"""
    print(f"ğŸ” è§£æè¯¦æƒ…é¡µé™„ä»¶...")
    try:
        resp = requests.get(url, headers=HEADERS, verify=False, timeout=10)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        all_links = soup.find_all('a')
        
        # 1. æ”¶é›†æ‰€æœ‰ Excel å€™é€‰é“¾æ¥
        candidates = []
        for a in all_links:
            href = a.get('href')
            if not href: continue
            
            if href.lower().endswith(('.xls', '.xlsx')):
                full_url = urljoin(url, href)
                name = a.get_text(strip=True)
                if not name.lower().endswith(('.xls', '.xlsx')):
                    name = os.path.basename(href)
                # æ¸…ç†æ–‡ä»¶å
                name = re.sub(r'[\\/*?:"<>|]', "", name)
                candidates.append({'name': name, 'url': full_url})

        if not candidates:
            print("âš ï¸ æœªå‘ç° Excel é™„ä»¶ã€‚")
            return

        # 2. æ™ºèƒ½ç­›é€‰é™„ä»¶
        student_files = [f for f in candidates if is_student_file(f['name'])]
        
        final_targets = []
        if student_files:
            print(f"ğŸ¯ æ£€æµ‹åˆ° {len(student_files)} ä¸ªå­¦ç”Ÿä¸“ç”¨æ–‡ä»¶ï¼Œä»…ä¸‹è½½è¿™äº›ã€‚")
            final_targets = student_files
        else:
            print("â„¹ï¸ æœªæ£€æµ‹åˆ°æ˜ç¡®çš„'å­¦ç”Ÿç‰ˆ'æ–‡ä»¶ï¼Œå°†ä¸‹è½½æ‰€æœ‰éç›‘è€ƒæ–‡ä»¶ã€‚")
            final_targets = [f for f in candidates if not is_teacher_file(f['name'])]

        import tempfile
        import shutil
        import hashlib

        # 3. ä¸‹è½½åˆ°ä¸´æ—¶ç›®å½•
        with tempfile.TemporaryDirectory() as temp_dir:
            print(f"â³ ä¸‹è½½åˆ°ä¸´æ—¶ç›®å½•: {temp_dir}")
            downloaded_files = []
            
            count = 0
            for file_info in final_targets:
                save_path = os.path.join(temp_dir, file_info['name'])
                if download_file(file_info['url'], save_path):
                    count += 1
                    downloaded_files.append(file_info['name'])
            
            if count == 0:
                print("âŒ æ²¡æœ‰æˆåŠŸä¸‹è½½ä»»ä½•æ–‡ä»¶ã€‚")
                return

            # 4. Idempotency Check (æ¯”å¯¹ hash)
            should_update = False
            
            if not os.path.exists(SAVE_DIR):
                should_update = True
                print("âœ¨ é¦–æ¬¡è¿è¡Œï¼Œå‡†å¤‡ä¿å­˜ã€‚")
            else:
                # è·å–ç°æœ‰ Excel æ–‡ä»¶
                existing_files = sorted([f for f in os.listdir(SAVE_DIR) if f.endswith(('.xls', '.xlsx'))])
                new_files = sorted(downloaded_files)
                
                if existing_files != new_files:
                    should_update = True
                    print("ğŸ”„ æ–‡ä»¶åˆ—è¡¨å˜æ›´ï¼Œå‡†å¤‡æ›´æ–°ã€‚")
                else:
                    # æ–‡ä»¶åˆ—è¡¨ç›¸åŒï¼Œæ¯”å¯¹å†…å®¹ hash
                    for fname in new_files:
                         new_path = os.path.join(temp_dir, fname)
                         old_path = os.path.join(SAVE_DIR, fname)
                         
                         with open(new_path, 'rb') as f1, open(old_path, 'rb') as f2:
                             if hashlib.md5(f1.read()).hexdigest() != hashlib.md5(f2.read()).hexdigest():
                                 should_update = True
                                 print(f"ğŸ”„ æ–‡ä»¶å†…å®¹å˜æ›´: {fname}")
                                 break
            
            if not should_update:
                print("âš¡ å†…å®¹æœªå˜æ›´ï¼Œè·³è¿‡æ›´æ–° (Idempotent)ã€‚")
                return

            # 5. æ‰§è¡Œæ›´æ–°
            if not os.path.exists(SAVE_DIR):
                os.makedirs(SAVE_DIR)
            
            # æ¸…ç†æ—§ Excel
            print("ğŸ§¹ æ¸…ç†æ—§æ•°æ®æ–‡ä»¶...")
            for f in os.listdir(SAVE_DIR):
                if f.endswith(('.xls', '.xlsx')):
                    try:
                        os.remove(os.path.join(SAVE_DIR, f))
                    except Exception as e:
                        print(f"   âŒ åˆ é™¤å¤±è´¥ {f}: {e}")

            # ç§»åŠ¨æ–°æ–‡ä»¶
            for fname in downloaded_files:
                src = os.path.join(temp_dir, fname)
                dst = os.path.join(SAVE_DIR, fname)
                shutil.copy2(src, dst)
                print(f"âœ… ä¿å­˜æ–‡ä»¶: {fname}")

            # ä¿å­˜ Metadata
            import json
            from datetime import datetime, timezone, timedelta
            
            beijing_tz = timezone(timedelta(hours=8))
            now_beijing = datetime.now(timezone.utc).astimezone(beijing_tz)

            metadata = {
                "source_url": url,
                "source_title": title,
                "downloaded_files": downloaded_files,
                "updated_at": now_beijing.isoformat()
            }
            meta_path = os.path.join(SAVE_DIR, "source_metadata.json")
            with open(meta_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å…ƒæ•°æ®å·²æ›´æ–°: {meta_path}")

        print(f"\nğŸ‰ å¤„ç†å®Œæ¯•ï¼æˆåŠŸåŒæ­¥ {count} ä¸ªæ–‡ä»¶ã€‚")
            
    except Exception as e:
        print(f"âŒ è¯¦æƒ…é¡µè§£æå¤±è´¥: {e}")

if __name__ == "__main__":
    print("=== NJUPT è€ƒè¯•å®‰æ’è‡ªåŠ¨åŒæ­¥å·¥å…· ===")
    result = find_latest_schedule_notification()
    if result:
        url, title = result
        process_detail_page(url, title)
    else:
        print("æœªè¿›è¡Œä»»ä½•æ›´æ–°ã€‚")