import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import os
import re
import urllib3

# ç¦ç”¨ SSL è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- é…ç½®åŒºåŸŸ ---
LIST_URL = "https://jwc.njupt.edu.cn/1594/list.htm"
SAVE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "public", "data")

# 1. å¿…é¡»åŒ…å«çš„å…³é”®è¯ (ä¸”å…³ç³»)
REQUIRED_KEYWORDS = ["å­¦å¹´", "å­¦æœŸ"]
# 2. å¿…é¡»åŒ…å«å…¶ä¸­ä¹‹ä¸€çš„å…³é”®è¯ (æˆ–å…³ç³»)
TARGET_KEYWORDS = ["è€ƒè¯•å®‰æ’è¡¨", "æœŸæœ«è€ƒè¯•", "è¯¾ç¨‹ç»“æŸè€ƒè¯•"]
# 3. ç»å¯¹ä¸èƒ½åŒ…å«çš„å…³é”®è¯ (æ’é™¤å™ªéŸ³)
EXCLUDE_KEYWORDS = [
    "é˜¶æ®µæ€§", "è¡¥è€ƒ", "æ¸…æ¬ ", "åˆ†çº§", "è¡¥å­¦", "æ¢è¯", 
    "é‡ä¿®", "é€‰æ‹”", "ç«èµ›", "å‘è½¦", "ç›‘è€ƒ"
]

# ä¼ªè£…è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://jwc.njupt.edu.cn/"
}

def is_valid_title(title):
    """
    åˆ¤æ–­æ ‡é¢˜æ˜¯å¦ä¸ºæ­£è§„æœŸæœ«è€ƒè¯•å®‰æ’
    """
    # 1. æ£€æŸ¥æ’é™¤è¯
    for kw in EXCLUDE_KEYWORDS:
        if kw in title:
            return False
    
    # 2. æ£€æŸ¥å¿…é¡»åŒ…å«çš„è¯
    for kw in REQUIRED_KEYWORDS:
        if kw not in title:
            return False
            
    # 3. æ£€æŸ¥ç›®æ ‡è¯ (è‡³å°‘å‘½ä¸­ä¸€ä¸ª)
    if not any(kw in title for kw in TARGET_KEYWORDS):
        return False
        
    return True

def is_student_file(filename):
    """åˆ¤æ–­æ˜¯å¦ä¸ºå­¦ç”Ÿç”¨è¡¨"""
    return "å­¦ç”Ÿ" in filename

def is_teacher_file(filename):
    """åˆ¤æ–­æ˜¯å¦ä¸ºæ•™å¸ˆ/ç›‘è€ƒè¡¨"""
    keywords = ["ç›‘è€ƒ", "æ•™å¸ˆ", "å·¡è€ƒ", "æ•™åŠ¡å‘˜"]
    return any(kw in filename for kw in keywords)

def download_file(url, save_path):
    try:
        print(f"  â¬‡ï¸  ä¸‹è½½ä¸­: {os.path.basename(save_path)} ...", end="", flush=True)
        response = requests.get(url, headers=HEADERS, verify=False, timeout=30)
        response.raise_for_status()
        with open(save_path, 'wb') as f:
            f.write(response.content)
        print(" [å®Œæˆ]")
        return True
    except Exception as e:
        print(f" [å¤±è´¥] {e}")
        return False

def find_latest_schedule_notification():
    """éå†åˆ—è¡¨é¡µï¼Œå¯»æ‰¾æœ€æ–°çš„ã€ç¬¦åˆé€»è¾‘çš„é€šçŸ¥"""
    print(f"ğŸ” è®¿é—®é€šçŸ¥åˆ—è¡¨: {LIST_URL}")
    try:
        resp = requests.get(LIST_URL, headers=HEADERS, verify=False, timeout=10)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        container = soup.select_one('div.col_news_con')
        if not container:
            print("âŒ æœªæ‰¾åˆ°åˆ—è¡¨å®¹å™¨ï¼Œè¯·æ£€æŸ¥é€‰æ‹©å™¨ã€‚")
            return None

        news_items = container.select('li.news')
        
        for item in news_items:
            # æå–æ ‡é¢˜
            title_span = item.select_one('span.news_title')
            a_tag = title_span.find('a') if title_span else item.find('a')
            
            if not a_tag: continue
            
            title = a_tag.get('title') or a_tag.get_text(strip=True)
            link = urljoin(LIST_URL, a_tag.get('href'))
            
            # ä½¿ç”¨å¢å¼ºçš„é€»è¾‘åˆ¤æ–­æ ‡é¢˜
            if is_valid_title(title):
                print(f"âœ… å‘½ä¸­ç›®æ ‡: [{title}]")
                print(f"ğŸ”— é“¾æ¥åœ°å€: {link}")
                return link, title
            else:
                # å¼€å¯æ­¤è¡Œå¯æŸ¥çœ‹è¢«è¿‡æ»¤æ‰çš„æ ‡é¢˜ï¼ˆè°ƒè¯•ç”¨ï¼‰
                # print(f"   è·³è¿‡: {title}")
                pass
        
        print("âš ï¸ æœªåœ¨é¦–é¡µæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æœŸæœ«è€ƒè¯•é€šçŸ¥ã€‚")
        return None
        
    except Exception as e:
        print(f"âŒ åˆ—è¡¨è·å–å¤±è´¥: {e}")
        return None

def process_detail_page(url, title):
    """è§£æè¯¦æƒ…é¡µå¹¶æ™ºèƒ½ä¸‹è½½é™„ä»¶"""
    print(f"ğŸ” è§£æè¯¦æƒ…é¡µé™„ä»¶...")
    try:
        resp = requests.get(url, headers=HEADERS, verify=False, timeout=10)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        all_links = soup.find_all('a')
        
        # 1. æ”¶é›†æ‰€æœ‰ Excel å€™é€‰é“¾æ¥
        candidates = []
        for a in all_links:
            href = a.get('href')
            if not href: continue
            
            if href.lower().endswith(('.xls', '.xlsx')):
                full_url = urljoin(url, href)
                name = a.get_text(strip=True)
                if not name.lower().endswith(('.xls', '.xlsx')):
                    name = os.path.basename(href)
                # æ¸…ç†æ–‡ä»¶å
                name = re.sub(r'[\\/*?:"<>|]', "", name)
                candidates.append({'name': name, 'url': full_url})

        if not candidates:
            print("âš ï¸ æœªå‘ç° Excel é™„ä»¶ã€‚")
            return

        # 2. æ™ºèƒ½ç­›é€‰é™„ä»¶
        # ç­–ç•¥: å¦‚æœæœ‰å« "å­¦ç”Ÿ" çš„æ–‡ä»¶ï¼Œåªä¸‹è½½è¿™äº›ã€‚å¦åˆ™ä¸‹è½½æ‰€æœ‰é "ç›‘è€ƒ" çš„æ–‡ä»¶ã€‚
        student_files = [f for f in candidates if is_student_file(f['name'])]
        
        final_targets = []
        if student_files:
            print(f"ğŸ¯ æ£€æµ‹åˆ° {len(student_files)} ä¸ªå­¦ç”Ÿä¸“ç”¨æ–‡ä»¶ï¼Œä»…ä¸‹è½½è¿™äº›ã€‚")
            final_targets = student_files
        else:
            print("â„¹ï¸ æœªæ£€æµ‹åˆ°æ˜ç¡®çš„'å­¦ç”Ÿç‰ˆ'æ–‡ä»¶ï¼Œå°†ä¸‹è½½æ‰€æœ‰éç›‘è€ƒæ–‡ä»¶ã€‚")
            final_targets = [f for f in candidates if not is_teacher_file(f['name'])]

        # 3. æ‰§è¡Œä¸‹è½½
        if not os.path.exists(SAVE_DIR):
            os.makedirs(SAVE_DIR)
        else:
            # [Added] Clean up old Excel files to prevent duplicates
            print("ğŸ§¹ æ¸…ç†æ—§æ•°æ®æ–‡ä»¶...")
            for f in os.listdir(SAVE_DIR):
                if f.endswith('.xlsx') or f.endswith('.xls'):
                    try:
                        os.remove(os.path.join(SAVE_DIR, f))
                        print(f"   - åˆ é™¤: {f}")
                    except Exception as e:
                        print(f"   âŒ åˆ é™¤å¤±è´¥ {f}: {e}")
            
        count = 0
        downloaded_files = []
        for file_info in final_targets:
            save_path = os.path.join(SAVE_DIR, file_info['name'])
            if download_file(file_info['url'], save_path):
                count += 1
                downloaded_files.append(file_info['name'])
        
        if count > 0:
            import json
            from datetime import datetime
            
            metadata = {
                "source_url": url,
                "source_title": title,
                "downloaded_files": downloaded_files,
                "updated_at": datetime.now().isoformat()
            }
            meta_path = os.path.join(SAVE_DIR, "source_metadata.json")
            with open(meta_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜: {meta_path}")

        print(f"\nğŸ‰ å¤„ç†å®Œæ¯•ï¼æˆåŠŸä¸‹è½½ {count} ä¸ªæ–‡ä»¶ã€‚")
            
    except Exception as e:
        print(f"âŒ è¯¦æƒ…é¡µè§£æå¤±è´¥: {e}")

if __name__ == "__main__":
    print("=== NJUPT è€ƒè¯•å®‰æ’è‡ªåŠ¨åŒæ­¥å·¥å…· ===")
    result = find_latest_schedule_notification()
    if result:
        url, title = result
        process_detail_page(url, title)
    else:
        print("æœªè¿›è¡Œä»»ä½•æ›´æ–°ã€‚")